# -*- coding: utf-8 -*-
"""madeamtextclassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Bj9GGrQ25dN-xPg0oaSi-1u6W8V4Snn
"""

'''
[X]- find dataset (imdb sentiment analysis)
[X]- divide into train/test
[X]- create graph showing distribution of target classes
[X]- describe dataset and what the model should be able to predict
- using sklearn, try:
  [X]- naive bayes
  [X]- logistic regression
  [X]- neural networks
- write up analysis of performance of various approaches
- accuracy does not determine grade -> quality of analysis determines grade
'''

import csv
import pandas as pd
# first row is review and second row is label (0 = negative, 1 = positive)
#df = pd.read_csv('movie.csv', header=0, usecols=[1,2], quoting=csv.QUOTE_NONE, encoding='utf-8') #'latin-1')
df = pd.read_csv('movie.csv')
print('rows and columns:', df.shape)
df.head()

# text preprocessing
import nltk
nltk.download('stopwords')

# clean data (no duplicates and NaN)
df.drop_duplicates(inplace=True)
no_of_nan_values=df.isna().sum().sum()
print(no_of_nan_values)

df['text_word_count']=df['text'].apply(lambda x:len(x.split()))
print(type(df['text_word_count']))
print(df['text_word_count'])

# text preprocessing
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

stopwords = set(stopwords.words('english'))
#vectorizer = TfidfVectorizer(stop_words=list(stopwords))
vectorizer_b = TfidfVectorizer(stop_words=list(stopwords), binary=True)
# .{column names}
X = df.text   #features
print(type(X))
Y = df.label  #targets
print(type(Y))
X.head()

# divide into train/test
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, train_size=0.75, random_state=1234)
X_train.shape

# apply tfidf vectorizer
X_train_list = X_train.values.astype('U').tolist()
X_test_list = X_test.values.astype('U').tolist()
X_train = vectorizer_b.fit_transform(X_train_list)  # fit and transform the train data
X_test = vectorizer_b.transform(X_test_list)        # transform only the test data

import seaborn as sb
import matplotlib.pyplot as plt

plt.figure(figsize=(10,10))
#colors=['#AB47BC','#6495ED']
colors = [sb.color_palette('pastel')[0], sb.color_palette('pastel')[4]]
plt.pie(df['label'].value_counts(),labels=['Positive','Negative'],autopct='%.1f%%',colors=colors);
plt.ylabel('Movie Sentiment');

"""## Describe dataset and what the model should be able to predict:

This dataset contains movie reviews from IMDB and consists of two columns: text and label. The text column contains the text of the review and the label column either has a 0 for a negative label or a 1 for a positive label. This model should be able to predict the sentiment of movie reviews on IMDB.

# Naive Bayes
"""

from sklearn.naive_bayes import MultinomialNB
#from sklearn.naive_bayes import BernoulliNB

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train, Y_train)
# naive_bayes2 = BernoulliNB()
# naive_bayes2.fit(X_train, Y_train)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# make predictions on the test data
pred = naive_bayes.predict(X_test)

# print confusion matrix
# confusion matrix has this form
#     tp   fp
#     fn   tn
print(confusion_matrix(Y_test, pred))

from sklearn.metrics import classification_report
print(classification_report(Y_test, pred))

# print('negative size in test data:',Y_test[Y_test==0].shape[0])
# print('test size: ', len(Y_test))
baseline = Y_test[Y_test==0].shape[0] / Y_test.shape[0] 
print("baseline: " + str(baseline))

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss
# set up X and Y
X = df.text
y = df.label

# divide into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, train_size=0.8, random_state=1234)

# vectorizer
vectorizer = TfidfVectorizer(binary=True)
X_train = vectorizer.fit_transform(X_train)  # fit and transform the train data
X_test = vectorizer.transform(X_test)        # transform only the test data

#train
classifier = LogisticRegression(solver='lbfgs', class_weight='balanced')
classifier.fit(X_train, Y_train)

# evaluate
pred = classifier.predict(X_test)
print('accuracy score: ', accuracy_score(Y_test, pred))
print('precision score: ', precision_score(Y_test, pred))
print('recall score: ', recall_score(Y_test, pred))
print('f1 score: ', f1_score(Y_test, pred))
probs = classifier.predict_proba(X_test)
print('log loss: ', log_loss(Y_test, probs))

"""# Neural Network"""

# set up X and Y
X = vectorizer_b.fit_transform(df.text)
Y = df.label

from sklearn.neural_network import MLPClassifier

# divide into train and test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, train_size=0.75, random_state=1234)

classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,
                   hidden_layer_sizes=(15, 2), random_state=1)
classifier.fit(X_train, Y_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
pred = classifier.predict(X_test)
print('accuracy score: ', accuracy_score(Y_test, pred))
print('precision score: ', precision_score(Y_test, pred))
print('recall score: ', recall_score(Y_test, pred))
print('f1 score: ', f1_score(Y_test, pred))

"""This neural network has a higher f1 and accuracy than the naive bayes, but lower than the logistic regression."""