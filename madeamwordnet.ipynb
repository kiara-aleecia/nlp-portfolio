{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saG-hDlVX3Tl",
        "outputId": "bdc5979d-c618-4365-e96d-008d9033bacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "import nltk\n",
        "import math\n",
        "nltk.download('popular')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.book import *\n",
        "text4\n",
        "'''\n",
        "TO DO:\n",
        "- write 2-3 sentence summary of wordnet\n",
        "- select noun, output all synsets\n",
        "  - select one synset from list of synsets \n",
        "    - extract definition, usage examples, lemmas\n",
        "    - from selected synset, traverse up wordnet hierarchy and output as you go\n",
        "    - write a couple of sentences of observations about how wordnet organizes nouns\n",
        "  - output the following (or empty list):\n",
        "    - hypernyms\n",
        "    - hyponyms\n",
        "    - meronyms\n",
        "    - holonyms\n",
        "    - antonyms\n",
        "- select verb, output all synsets\n",
        "  - extract definition, usage examples, lemmas\n",
        "    - from selected synset, traverse up wordnet hierarchy and output as you go\n",
        "    - write a couple of sentences of observations about how wordnet organizes verbs\n",
        "    - use morphy to find as many different forms of the word as you can\n",
        "- select two words you think might be similar\n",
        "  - find specific synsets you are interested in\n",
        "  - run wu-palmer similarity metric and lesk algorithm\n",
        "  - write a couple of sentences with your observations\n",
        "- write a couple of sentences about sentiwordnet describing\n",
        "  - functionality\n",
        "  - possible use cases\n",
        "- select emotionally charged word\n",
        "  - find sentisynsets\n",
        "  - output each polarity score\n",
        "- make up a sentence\n",
        "  - output polarity for each word in the sentence\n",
        "  - write a couple of sentences about your observations of the scores and the\n",
        "    utility of knowing these scores in an NLP application\n",
        "- write a couple of sentences about what a collocation is\n",
        "  - output collocations for text4 (inaugural corpus) -> part of nltk\n",
        "  - select a collocation identified by nltk\n",
        "  - calculate mutual information\n",
        "  - commentate on the results of the mutual info formula and your interpretation\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "BHYwMBf_l7EN",
        "outputId": "f4209c9b-071a-4058-b39f-f157f5661715"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTO DO:\\n- write 2-3 sentence summary of wordnet\\n- select noun, output all synsets\\n  - select one synset from list of synsets \\n    - extract definition, usage examples, lemmas\\n    - from selected synset, traverse up wordnet hierarchy and output as you go\\n    - write a couple of sentences of observations about how wordnet organizes nouns\\n  - output the following (or empty list):\\n    - hypernyms\\n    - hyponyms\\n    - meronyms\\n    - holonyms\\n    - antonyms\\n- select verb, output all synsets\\n  - extract definition, usage examples, lemmas\\n    - from selected synset, traverse up wordnet hierarchy and output as you go\\n    - write a couple of sentences of observations about how wordnet organizes verbs\\n    - use morphy to find as many different forms of the word as you can\\n- select two words you think might be similar\\n  - find specific synsets you are interested in\\n  - run wu-palmer similarity metric and lesk algorithm\\n  - write a couple of sentences with your observations\\n- write a couple of sentences about sentiwordnet describing\\n  - functionality\\n  - possible use cases\\n- select emotionally charged word\\n  - find sentisynsets\\n  - output each polarity score\\n- make up a sentence\\n  - output polarity for each word in the sentence\\n  - write a couple of sentences about your observations of the scores and the\\n    utility of knowing these scores in an NLP application\\n- write a couple of sentences about what a collocation is\\n  - output collocations for text4 (inaugural corpus) -> part of nltk\\n  - select a collocation identified by nltk\\n  - calculate mutual information\\n  - commentate on the results of the mutual info formula and your interpretation\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 1 :)\n",
        "Write 2-3 sentence summary of wordnet:\n",
        "\n",
        "- WordNet groups words together based on their meanings and each synset represents a distinct concept. Words in this lexical database are related with synonymy/antonymy, hyperonomy/hyponymy, and meronymy/holonymy which makes semantic analysis easier due to words in close proximity to each other in the \"word net\" being made unambiguous. "
      ],
      "metadata": {
        "id": "ni_QyWzEFrq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 2, 3, 4 :)"
      ],
      "metadata": {
        "id": "-dEz2NsLQ--J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "[X]- select noun, output all synsets\n",
        "  [X]- select one synset from list of synsets \n",
        "    [X]- extract definition, usage examples, lemmas\n",
        "    [X]- from selected synset, traverse up wordnet hierarchy and output as you go\n",
        "    [X]- write a couple of sentences of observations about how wordnet organizes nouns\n",
        "  [X]- output the following (or empty list):\n",
        "    [X]- hypernyms\n",
        "    [X]- hyponyms\n",
        "    [X]- meronyms\n",
        "    [X]- holonyms\n",
        "    [X]- antonyms\n",
        "'''\n",
        "# run cell 2 (from nltk.corpus import wordnet as wn)\n",
        "\n",
        "noun = \"car\"\n",
        "noun_synset = wn.synsets(noun)\n",
        "noun_definition = wn.synset('car.n.02').definition()\n",
        "noun_example = wn.synset('car.n.02').examples()\n",
        "noun_lemma = wn.synset('car.n.02').lemmas()\n",
        "\n",
        "print(f\"Noun: {noun}\")\n",
        "print(f\"Synset: {noun_synset}\")\n",
        "print(f\"Definition: {noun_definition}\")\n",
        "print(f\"Example: {noun_example}\")\n",
        "print(f\"Lemmas: {noun_lemma}\")\n",
        "# a hyponym of a given word has an IS A relationship with it\n",
        "# meronym is the part of something\n",
        "# holonym is the whole\n",
        "# traverse up wordnet hierarchy and output as you go\n",
        "car = wn.synset('car.n.02')\n",
        "hyper = lambda s: s.hypernyms()\n",
        "hypo = lambda s: s.hyponyms()\n",
        "mero = lambda s: s.member_meronyms()\n",
        "holo = lambda s: s.member_holonyms()\n",
        "ant = noun_lemma[0].antonyms()\n",
        "print(f\"Car Hypernyms: {list(car.closure(hyper))}\")\n",
        "print(f\"Car Hyponyms: {list(car.closure(hypo))}\")\n",
        "print(f\"Car Meronyms: {list(car.closure(mero))}\")\n",
        "print(f\"Car Holonyms: {list(car.closure(holo))}\")\n",
        "print(f\"Car Antonyms: {ant}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBIFIWlnJQOr",
        "outputId": "b42ab220-82dd-4cb2-9a39-be8c851c5820"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun: car\n",
            "Synset: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n",
            "Definition: a wheeled vehicle adapted to the rails of railroad\n",
            "Example: ['three cars had jumped the rails']\n",
            "Lemmas: [Lemma('car.n.02.car'), Lemma('car.n.02.railcar'), Lemma('car.n.02.railway_car'), Lemma('car.n.02.railroad_car')]\n",
            "Car Hypernyms: [Synset('wheeled_vehicle.n.01'), Synset('container.n.01'), Synset('vehicle.n.01'), Synset('instrumentality.n.03'), Synset('conveyance.n.03'), Synset('artifact.n.01'), Synset('whole.n.02'), Synset('object.n.01'), Synset('physical_entity.n.01'), Synset('entity.n.01')]\n",
            "Car Hyponyms: [Synset('baggage_car.n.01'), Synset('cabin_car.n.01'), Synset('club_car.n.01'), Synset('freight_car.n.01'), Synset('guard's_van.n.01'), Synset('handcar.n.01'), Synset('mail_car.n.01'), Synset('passenger_car.n.01'), Synset('slip_coach.n.01'), Synset('tender.n.04'), Synset('van.n.03'), Synset('boxcar.n.01'), Synset('cattle_car.n.01'), Synset('coal_car.n.01'), Synset('flatcar.n.01'), Synset('gondola_car.n.01'), Synset('refrigerator_car.n.01'), Synset('tank_car.n.01'), Synset('dining_car.n.01'), Synset('nonsmoker.n.02'), Synset('parlor_car.n.01'), Synset('pullman.n.01'), Synset('sleeping_car.n.01'), Synset('smoker.n.03'), Synset('stockcar.n.01')]\n",
            "Car Meronyms: []\n",
            "Car Holonyms: [Synset('train.n.01')]\n",
            "Car Antonyms: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a couple of sentences of observations about how wordnet organizes nouns:\n",
        "\n",
        "- In WordNet, all nouns can be traced back to the same parent noun: entity. All the hyponyms of a given noun are more specific instances of that noun and the given noun itself is a more specific instance of its hypernyms. Nouns are more likely to have meronyms and holonyms than antonyms."
      ],
      "metadata": {
        "id": "lE7hvDKjLYou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 5, 6, 7 :)"
      ],
      "metadata": {
        "id": "IvECfC-JRMWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "[X]- select verb, output all synsets\n",
        "  [X]- extract definition, usage examples, lemmas\n",
        "    [X]- from selected synset, traverse up wordnet hierarchy and output as you go\n",
        "    [X]- write a couple of sentences of observations about how wordnet organizes verbs\n",
        "    [X]- use morphy to find as many different forms of the word as you can\n",
        "'''\n",
        "\n",
        "# run cell 2 (from nltk.corpus import wordnet as wn)\n",
        "\n",
        "verb = \"see\"\n",
        "verb_synset = wn.synsets(verb)\n",
        "verb_definition = wn.synset('see.v.01').definition()\n",
        "verb_example = wn.synset('see.v.01').examples()\n",
        "verb_lemma = wn.synset('see.v.01').lemmas()\n",
        "\n",
        "print(f\"Verb: {verb}\")\n",
        "print(f\"Synset: {verb_synset}\")\n",
        "print(f\"Definition: {verb_definition}\")\n",
        "print(f\"Example: {verb_example}\")\n",
        "print(f\"Lemmas: {verb_lemma}\")\n",
        "\n",
        "# traverse up wordnet hierarchy and output as you go\n",
        "see = wn.synset('see.v.01')\n",
        "hyper = lambda s: s.hypernyms()\n",
        "hypo = lambda s: s.hyponyms()\n",
        "mero = lambda s: s.member_meronyms()\n",
        "holo = lambda s: s.member_holonyms()\n",
        "ant = verb_lemma[0].antonyms()\n",
        "print(f\"Car Hypernyms: {list(see.closure(hyper))}\")\n",
        "print(f\"Car Hyponyms: {list(see.closure(hypo))}\")\n",
        "print(f\"Car Meronyms: {list(see.closure(mero))}\")\n",
        "print(f\"Car Holonyms: {list(see.closure(holo))}\")\n",
        "print(f\"Car Antonyms: {ant}\")\n",
        "\n",
        "print(f\"As an adjective w/ Morphy: {wn.morphy('see', wn.ADJ)}\")\n",
        "print(f\"As a noun w/ Morphy: {wn.morphy('see', wn.NOUN)}\")\n",
        "print(f\"As a verb w/ Morphy: {wn.morphy('see', wn.VERB)}\")\n",
        "print(f\"As an adverb w/ Morphy: {wn.morphy('see', wn.ADV)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp9sEp5ekKfr",
        "outputId": "9bf0324a-cd56-48b3-f0b5-b236d7101301"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verb: see\n",
            "Synset: [Synset('see.n.01'), Synset('see.v.01'), Synset('understand.v.02'), Synset('witness.v.02'), Synset('visualize.v.01'), Synset('see.v.05'), Synset('learn.v.02'), Synset('watch.v.03'), Synset('meet.v.01'), Synset('determine.v.08'), Synset('see.v.10'), Synset('see.v.11'), Synset('see.v.12'), Synset('visit.v.01'), Synset('attend.v.02'), Synset('see.v.15'), Synset('go_steady.v.01'), Synset('see.v.17'), Synset('see.v.18'), Synset('see.v.19'), Synset('examine.v.02'), Synset('experience.v.01'), Synset('see.v.22'), Synset('see.v.23'), Synset('interpret.v.01')]\n",
            "Definition: perceive by sight or have the power to perceive by sight\n",
            "Example: ['You have to be a good observer to see all the details', 'Can you see the bird in that tree?', 'He is blind--he cannot see']\n",
            "Lemmas: [Lemma('see.v.01.see')]\n",
            "Car Hypernyms: [Synset('perceive.v.01')]\n",
            "Car Hyponyms: [Synset('behold.v.01'), Synset('catch_sight.v.01'), Synset('glimpse.v.01'), Synset('see.v.17')]\n",
            "Car Meronyms: []\n",
            "Car Holonyms: []\n",
            "Car Antonyms: []\n",
            "As an adjective w/ Morphy: None\n",
            "As a noun w/ Morphy: see\n",
            "As a verb w/ Morphy: see\n",
            "As an adverb w/ Morphy: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a couple of sentences of observations about how wordnet organizes verbs:\n",
        "- It seems that WordNet organizes verbs differently than nouns because not all verbs have the same common parent. Hypernyms seem to be broader concepts of the verb and hyponyms are more specific concepts of the verb. Both categories seem to be also synonyms of whatever verb that is chosen for analysis.\n"
      ],
      "metadata": {
        "id": "EVVKztvgRgRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 8 :)"
      ],
      "metadata": {
        "id": "Sfve9vULakN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "[X]- select two words you think might be similar\n",
        "  [X]- find specific synsets you are interested in\n",
        "  [x]- run wu-palmer similarity metric and lesk algorithm\n",
        "  [X]- write a couple of sentences with your observations\n",
        "'''\n",
        "# run cell 2 (from nltk.wsd import lesk)\n",
        "\n",
        "bottle = wn.synset('bottle.n.01')\n",
        "cup = wn.synset('cup.n.01')\n",
        "#run = wn.synset('run.v.01')\n",
        "#catch = wn.synset('catch.v.01')\n",
        "#wn.path_similarity(bottle, cup)\n",
        "\n",
        "# based on depth of two senses in taxonomy and of most specific common ancestor node\n",
        "wup_sim = wn.wup_similarity(bottle, cup)\n",
        "#wup_sim1 = wn.wup_similarity(run, catch)\n",
        "print(f\"Wu-Palmer Similarity: {wup_sim}\")\n",
        "#print(f\"Wu-Palmer Similarity: {wup_sim1}\")\n",
        "\n",
        "# returns synset w/ highest number of overlapping words btwn context sentence\n",
        "# and definitions in each synset for the target word\n",
        "sent = ['Give', 'the', 'baby', 'her', 'bottle', '!']\n",
        "print(f\"Lesk Algorithm (bottle): {lesk(sent, 'bottle', pos='n')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8GIDRNvVXhC",
        "outputId": "4d4b26bc-774e-4bda-ff3b-5ba20aa55ef7"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wu-Palmer Similarity: 0.8235294117647058\n",
            "Lesk Algorithm (bottle): Synset('bottle.n.02')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a couple of sentences with your observations:\n",
        "- It seems that the Wu-Palmer similarity metric is higher when the words have more similar meanings or are related under a topic like sports activities. The Lesk algorithm uses sentence analysis to find a synset of a given word that has the most similar sentences in its synset as the sentence chosen by the programmer. With both of these, we can disambiguate the meaning of words in a sentence more easily by choosing the definition of words that score highest with these two methods instead of solely relying on our own contextual analysis."
      ],
      "metadata": {
        "id": "uxQW1dv3evGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 9 :)"
      ],
      "metadata": {
        "id": "8dQ3Td2bfCqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a couple of sentences about sentiwordnet describing: functionality, possible use cases\n",
        "- SentiWordNet is an added functionality to WordNet that gives positive, negative, and objective scores to every word. It has uses in the field of opinion mining, which is important for analysis of public opinions about important entities such as political candidates or businesses. It can also help anyone who feels that they struggle to accurately discern the meanings of subjective sentences.\n"
      ],
      "metadata": {
        "id": "eM4vn2EcxOrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "[X]- write a couple of sentences about sentiwordnet describing\n",
        "  [X]- functionality\n",
        "  [X]- possible use cases\n",
        "[X]- select emotionally charged word\n",
        "  [X]- find sentisynsets\n",
        "  [X]- output each polarity score\n",
        "[X]- make up a sentence\n",
        "  [X]- output polarity for each word in the sentence\n",
        "  - write a couple of sentences about your observations of the scores and the\n",
        "    utility of knowing these scores in an NLP application\n",
        "'''\n",
        "# run cell 2 (from nltk.corpus import sentiwordnet as swn)\n",
        "\n",
        "senti_list = list(swn.senti_synsets('satisfied')) \n",
        "for item in senti_list:\n",
        "  print(item)\n",
        "  print(\"Positive score = \", item.pos_score())\n",
        "  print(\"Negative score = \", item.neg_score())\n",
        "  print(\"Objective score = \", item.obj_score())\n",
        "\n",
        "print(\"\\n\")\n",
        "#sent = ['Have', 'a', 'great', 'day', ',', 'honey', '!']\n",
        "sent = 'have yourself a merry little christmas'\n",
        "neg = 0\n",
        "pos = 0\n",
        "tokens = sent.split()\n",
        "for token in tokens:\n",
        "  print(token)\n",
        "  syn_list = list(swn.senti_synsets(token))\n",
        "  if syn_list:\n",
        "    syn = syn_list[0]\n",
        "    neg += syn.neg_score()\n",
        "    pos += syn.pos_score()\n",
        "    #print(syn)\n",
        "    print(\"Positive score = \", syn.pos_score())\n",
        "    print(\"Negative score = \", syn.neg_score())\n",
        "    print(\"Objective score = \", syn.obj_score())\n",
        "print(\"neg\\tpos counts\")\n",
        "print(neg, '\\t', pos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PymKkpLMfFJg",
        "outputId": "6cc72aab-b1bd-4248-cae2-afc66c65bdef"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<satisfy.v.01: PosScore=0.5 NegScore=0.0>\n",
            "Positive score =  0.5\n",
            "Negative score =  0.0\n",
            "Objective score =  0.5\n",
            "<satisfy.v.02: PosScore=0.375 NegScore=0.0>\n",
            "Positive score =  0.375\n",
            "Negative score =  0.0\n",
            "Objective score =  0.625\n",
            "<meet.v.04: PosScore=0.125 NegScore=0.0>\n",
            "Positive score =  0.125\n",
            "Negative score =  0.0\n",
            "Objective score =  0.875\n",
            "<satisfied.s.01: PosScore=0.375 NegScore=0.0>\n",
            "Positive score =  0.375\n",
            "Negative score =  0.0\n",
            "Objective score =  0.625\n",
            "<quenched.s.01: PosScore=0.0 NegScore=0.0>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0\n",
            "\n",
            "\n",
            "have\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0\n",
            "yourself\n",
            "a\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0\n",
            "merry\n",
            "Positive score =  0.75\n",
            "Negative score =  0.125\n",
            "Objective score =  0.125\n",
            "little\n",
            "Positive score =  0.0\n",
            "Negative score =  0.25\n",
            "Objective score =  0.75\n",
            "christmas\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0\n",
            "neg\tpos counts\n",
            "0.375 \t 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a couple of sentences about your observations of the scores and the utility of knowing these scores in an NLP application:\n",
        "- Some of the scores given to the words I chose for the sentence and for the \"emotionally charged\" word surprised me because I was expecting either a higher positive score or a higher negative score. I realize that taking into account such a large dataset gives a broader sense of a word than my own singular perceptions of that word, so I believe that it is very useful to have a tool like this in an NLP application. Using something like SentiWordNet to know the scores of certain words would be helpful in a business or marketing field along with NLP because it would assist with market research, reputation management, and overall enhancement of customer experiences."
      ],
      "metadata": {
        "id": "v31gKZzNX4sR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 10 :)\n"
      ],
      "metadata": {
        "id": "uk7cZBMgYhWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a couple of sentences about what a collocation is:\n",
        "- A collocation is a combination of words that cannot convey a certain concept separately. Because of this, collocations can be found by analyzing the frequency at which two or more words are found together in a certain corpus, even if the words themselves are infrequent."
      ],
      "metadata": {
        "id": "eP-xbJ9813Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "[X]- write a couple of sentences about what a collocation is\n",
        "  [X]- output collocations for text4 (inaugural corpus) -> part of nltk\n",
        "  [X]- select a collocation identified by nltk\n",
        "  [X]- calculate mutual information\n",
        "  - commentate on the results of the mutual info formula and your interpretation\n",
        "'''\n",
        "# run cell 1 and 2 \n",
        "#import math\n",
        "#from nltk.book import *\n",
        "#text4\n",
        "text4.collocations()\n",
        "colloc = 'oneanother'\n",
        "# collocations: two words have a meaning greater than what the words can convey alone\n",
        "# mutual information: P(x,y) / [P(x) * P(y)]\n",
        "text = \"\".join(text4.tokens)\n",
        "\n",
        "vocab = len(set(text4))\n",
        "oa = text.count(colloc)/vocab\n",
        "print(\"p(one another) = \", oa)\n",
        "o = text.count('one')/vocab\n",
        "print(\"p(one) = \", o)\n",
        "a = text.count('another')/vocab\n",
        "print('p(another) = ', a)\n",
        "mut_info = math.log2(oa / (o * a))\n",
        "print('mutual information = ', mut_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6ztyxd9YlJV",
        "outputId": "73bf04be-6a44-4485-ad45-6c1468cca391"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n",
            "p(one another) =  0.0021945137157107233\n",
            "p(one) =  0.06733167082294264\n",
            "p(another) =  0.006683291770573566\n",
            "mutual information =  2.2859133524709767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commentate on the results of the mutual info formula and your interpretation:\n",
        "- The mutual information formula is useful for guessing if a group of words is a collocation or not because it takes into account the possible infrequency of the collocation itself. The algorithm shows how dependent two variables are on each other, so if two words together are a collocation, they are likely to score as highly dependent on each other."
      ],
      "metadata": {
        "id": "u2pEMdi83P0u"
      }
    }
  ]
}